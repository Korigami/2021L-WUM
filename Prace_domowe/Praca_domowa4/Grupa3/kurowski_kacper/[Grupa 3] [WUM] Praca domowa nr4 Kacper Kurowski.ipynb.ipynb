{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WUM] PD4 \n",
    "## Kacper Kurowski\n",
    "Wczytajmy paczki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, RandomizedSearchCV)\n",
    "from matplotlib import pyplot as plt\n",
    "from dalex.datasets import load_apartments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed = 42\n",
    "import category_encoders as ce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartments\n",
    "\n",
    "raz dane apartamentów. Przy okazji zakodujmy dystrykty przy pomocy zmiennej m2_price - załóżmy, że mniej więcej ceny apartamentów mocno zależą od tego, gdzie się znajdują"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = load_apartments()\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols=['district'])\n",
    "ap = target_encoder.fit_transform(ap, ap['m2_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podzielmy dane na zbiór testowy i treningowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(ap['m2_price'])\n",
    "X = ap.drop(['m2_price'],axis=1)\n",
    "\n",
    "ap_X_train, ap_X_test, ap_y_train, ap_y_test = train_test_split(X, y, test_size=0.25, random_state=1618)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skorzystajmy z SVM na nieprzeskalowanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "param_grid_SVM_rbf = {\n",
    "    'C': [1, 10, 50, 100, 150, 200, 300],\n",
    "    'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto', 0.5, 0.75, 1, 1.25, 1.5],    \n",
    "}\n",
    "\n",
    "svr_unscal_rbf = SVR()\n",
    "svr_unscal_grid_rbf = RandomizedSearchCV(\n",
    "    svr_unscal_rbf, param_grid_SVM_rbf,\n",
    "    scoring ='neg_root_mean_squared_error', n_iter=100, n_jobs=-1)\n",
    "svr_unscal_grid_rbf.fit(ap_X_train, ap_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = svr_unscal_grid_rbf.best_estimator_.predict(ap_X_test)\n",
    "print(\"RMSE dla nieprzeskalowanych:\", np.sqrt(mean_squared_error(ap_y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzyskaliśmy pewien wynik dla danych nieprzeskalowanych w metryce RMSE. Przeskalujmy następnie dane, by sprawdzić, czy przeskalowanie, którego użyli autorzy artykułu rzeczywiście poprawia uzyskiwane rezultaty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "ap_X_train_scal = ap_X_train.copy()\n",
    "ap_X_train_scal[ap_X_train_scal.columns.values] = ss.fit_transform(\n",
    "    ap_X_train_scal[ap_X_train_scal.columns.values]\n",
    ")\n",
    "ap_X_test_scal = ap_X_test.copy()\n",
    "ap_X_test_scal[ap_X_test_scal.columns.values] = ss.fit_transform(\n",
    "    ap_X_test_scal[ap_X_test_scal.columns.values]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scal_rbf = SVR()\n",
    "svr_scal_grid_rbf = RandomizedSearchCV(\n",
    "    svr_unscaled_rbf, param_grid_SVM_rbf,\n",
    "    scoring ='neg_root_mean_squared_error', n_iter=100, n_jobs=-1)\n",
    "svr_scal_grid_rbf.fit(ap_X_train_scal, ap_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_scal = svr_scal_grid_rbf.best_estimator_.predict(ap_X_test_scal)\n",
    "print(\"RMSE dla przeskalowanych:\", np.sqrt(mean_squared_error(ap_y_test, y_test_hat_scal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jak widzimy, przeskalowanie danych znacząco poprawiło uzsykiwane rezultaty - tym razem błąd wynosi jedynie 40% poprzednio uzyskanego. Oznacza to, że autorzy artykułu słusznie przeskaowali dane.\n",
    "\n",
    "## Aus Weather\n",
    "\n",
    "Dokonajmy następnie podobnej analizy na zbiorze weatherAUS, który juz w jednej z poprzednich prac domowych analizowaliśmy.\n",
    "W tamtej pracy domowej dokonaliśmy kodowania zmiennych, z którego korzystam poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_wheather = pd.read_csv( \"/home/kurowskik/kaggle/weatherAUS.csv\", sep = \",\", header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aus_wheather[\"Evaporation\"]\n",
    "del aus_wheather[\"Sunshine\"]\n",
    "del aus_wheather[\"Cloud9am\"]\n",
    "del aus_wheather[\"Cloud3pm\"]\n",
    "\n",
    "direction_to_encoding = {\n",
    "    \"N\" : [1.0,0.0,0.0,0.0],\n",
    "    \"NNW\" : [0.75,0.25,0.0,0.0],\n",
    "    \"NW\" : [0.5,0.5,0.0,0.0],\n",
    "    \"WNW\" : [0.25,0.66,0.0,0.0],\n",
    "    \"W\" : [0.0,1.0,0.0,0.0],\n",
    "    \"WSW\" : [0.0,0.75,0.25,0.0],\n",
    "    \"SW\" : [0.0,0.5,0.5,0.0],\n",
    "    \"SSW\" : [0.0,0.75,0.66,0.0],\n",
    "    \"S\" : [0.0,0.0,1.0,0.0],\n",
    "    \"SSE\" : [0.0,0.0,0.75,0.25],\n",
    "    \"SE\" : [0.0,0.0,0.5,0.5],\n",
    "    \"ESE\" : [0.0,0.0,0.25,0.75],\n",
    "    \"E\" : [0.0,0.0,0.0,1.0],\n",
    "    \"ENE\" : [0.25,0.0,0.0,0.75],\n",
    "    \"NE\" : [0.5,0.0,0.0,0.5],\n",
    "    \"NNE\" : [0.75,0.66,0.0,0.25],\n",
    "    \"nan\" : [0.0,0.0,0.0,0.0]\n",
    "}\n",
    "\n",
    "GustDir = pd.DataFrame( \n",
    "    aus_wheather[\"WindGustDir\"].fillna(\"nan\").map(direction_to_encoding).tolist(),\n",
    "    columns=['WindGustDirN','WindGustDirW','WindGustDirS','WindGustDirE'],\n",
    "    index = aus_wheather.index)\n",
    "aus_wheather = aus_wheather.merge(GustDir, left_index=True, right_index=True)\n",
    "\n",
    "GustDir9am = pd.DataFrame( \n",
    "    aus_wheather[\"WindDir9am\"].fillna(\"nan\").map(direction_to_encoding).tolist(),\n",
    "    columns=['WindDir9amN','WindDir9amW','WindDir9amS','WindDir9amE'],\n",
    "    index = aus_wheather.index)\n",
    "\n",
    "aus_wheather = aus_wheather.merge(GustDir9am, left_index=True, right_index=True)\n",
    "GustDir3pm = pd.DataFrame( \n",
    "    aus_wheather[\"WindDir3pm\"].fillna(\"nan\").map(direction_to_encoding).tolist(),\n",
    "    columns=['WindDir3pmN','WindDir3pmW','WindDir3pmS','WindDir3pmE'],\n",
    "    index = aus_wheather.index)\n",
    "aus_wheather = aus_wheather.merge(GustDir3pm, left_index=True, right_index=True)\n",
    "\n",
    "def encode_dates(x):\n",
    "    tmp = x.split(\"-\")\n",
    "    return [float( tmp[0]), float(tmp[1]), float(tmp[2]) ]\n",
    "\n",
    "dates = pd.DataFrame( \n",
    "    aus_wheather['Date'].map( encode_dates).tolist(),\n",
    "    columns=[\"Year\", \"Month\", \"Day\"],\n",
    "    index = aus_wheather.index)\n",
    "aus_wheather = aus_wheather.merge(dates, left_index=True, right_index=True)\n",
    "\n",
    "def encodeRain(x):\n",
    "    if x == \"Yes\":\n",
    "        return 1\n",
    "    elif x == \"No\":\n",
    "        return 0\n",
    "    \n",
    "aus_wheather['RainTomorrow'] = aus_wheather['RainTomorrow'].map( encodeRain)\n",
    "aus_wheather['RainToday'] = aus_wheather['RainToday'].map( encodeRain)\n",
    "\n",
    "tmp = aus_wheather['Location'].map( lambda x: sum(bytearray(x, 'utf-8'))+len(x) ) # Kodujemy lokację, niestety nieróżnowartościowo\n",
    "\n",
    "aus_wheather['Location'] = tmp\n",
    "\n",
    "del aus_wheather[\"Date\"]\n",
    "del aus_wheather[\"WindGustDir\"]\n",
    "del aus_wheather[\"WindDir9am\"]\n",
    "del aus_wheather[\"WindDir3pm\"]\n",
    "\n",
    "aus_wheather.fillna('0', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jako, że danych jest bardzo dużo, zdecydowałem się ograniczyć liczbę wierszy do 1000 - w ten sposób tyle samo wierszy ile w pierwszym zbiorze danych, więc obliczenia nie są przytłaczające."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_smaller = aus_wheather.sample( 1000, random_state = 1618)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(aus_smaller['RainTomorrow'])\n",
    "X = aus_smaller.drop(['RainTomorrow'],axis=1)\n",
    "\n",
    "\n",
    "aus_X_train, aus_X_test, aus_y_train, aus_y_test = train_test_split(X, y, test_size=0.25, random_state=1618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_SVM_poly = {\n",
    "    'kernel': ['poly'],\n",
    "    #'gamma': ['scale', 'auto', 0.5, 1, 1.5], niestety z tym nie potrafi przeliczyc...\n",
    "    'degree': [2, 3, 5, 6],\n",
    "    'C': [1, 10, 100, 150, 200, 300],\n",
    "}\n",
    "\n",
    "svr_unscal_poly = SVR()\n",
    "svr_unscal_grid_poly = RandomizedSearchCV(\n",
    "    svr_unscal_poly, param_grid_SVM_poly,\n",
    "    scoring ='neg_root_mean_squared_error', n_iter=50, n_jobs=-1)\n",
    "svr_unscal_grid_poly.fit(aus_X_train, aus_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_aus = svr_unscal_grid_poly.best_estimator_.predict(aus_X_test)\n",
    "print(\"RMSE dla nieprzeskalowanych:\", np.sqrt(mean_squared_error(aus_y_test, y_test_hat_aus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak w przypadku zbioru danych apartamentów, przeskalujmy dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "aus_X_train_scal = aus_X_train.copy()\n",
    "aus_X_train_scal[aus_X_train_scal.columns.values] = ss.fit_transform(\n",
    "    aus_X_train_scal[aus_X_train_scal.columns.values]\n",
    ")\n",
    "aus_X_test_scal = aus_X_test.copy()\n",
    "aus_X_test_scal[aus_X_test_scal.columns.values] = ss.fit_transform(\n",
    "    aus_X_test_scal[aus_X_test_scal.columns.values]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scal_poly = SVR()\n",
    "svr_scal_grid_poly = RandomizedSearchCV(\n",
    "    svr_scal_poly, param_grid_SVM_poly,\n",
    "    scoring ='neg_root_mean_squared_error', n_iter=50, n_jobs=-1)\n",
    "svr_scal_grid_poly.fit(aus_X_train_scal, aus_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_scal = svr_scal_grid_poly.best_estimator_.predict(aus_X_test_scal)\n",
    "print(\"RMSE dla przeskalowanych:\", np.sqrt(mean_squared_error(aus_y_test, y_test_hat_scal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym razem przeskaowanie nie daje tak dobrych rezultatów jak w przypadku zbioru danych apartments. Być może jest to kwestia liczby zmiennych, których w zbiorze ausWeather jest znacznie więcej"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
